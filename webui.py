#!/usr/bin/env python3
"""
è½»é‡çº§ Gradio WebUI for serve.py
ç›´æ¥è°ƒç”¨ serve.py çš„ FastAPI æ¥å£è¿›è¡Œæ¨ç†
"""

import os
import sys
import requests
import gradio as gr
import json
import subprocess
import random
import re
from typing import Optional, Iterator, Any

# é…ç½®
API_BASE_URL = os.environ.get("API_BASE_URL", "http://127.0.0.1:8000")
API_TIMEOUT = int(os.environ.get("API_TIMEOUT", "360"))
WEBUI_PORT = int(os.environ.get("WEBUI_PORT", "7860"))
WEBUI_HOST = os.environ.get("WEBUI_HOST", "0.0.0.0")
WEBUI_SHARE = os.environ.get("WEBUI_SHARE", "0") == "1"


def _pretty_json(obj: Any) -> str:
    try:
        return json.dumps(obj, ensure_ascii=False, indent=2)
    except Exception:
        return str(obj)


def check_api_health() -> tuple[bool, str]:
    """æ£€æŸ¥åç«¯ API å¥åº·çŠ¶æ€"""
    try:
        response = requests.get(f"{API_BASE_URL}/", timeout=10)
        if response.status_code == 200:
            data = response.json()
            status = data.get("status", "ok")
            return True, f"âœ… åç«¯çŠ¶æ€: {status}"
        else:
            return False, f"âŒ åç«¯è¿”å›é”™è¯¯: HTTP {response.status_code}"
    except requests.exceptions.ConnectionError:
        return False, f"âŒ æ— æ³•è¿æ¥åˆ°åç«¯ {API_BASE_URL}"
    except Exception as e:
        return False, f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {str(e)}"


def predict(user_input: str, gen_params: Optional[dict] = None) -> Iterator[str]:
    """è°ƒç”¨åç«¯ API è¿›è¡Œæ¨ç†"""
    if not isinstance(user_input, str):
        user_input = str(user_input)
    if not user_input or not user_input.strip():
        yield "âš ï¸ è¯·è¾“å…¥é—®é¢˜"
        return

    # æ£€æŸ¥ API å¯ç”¨æ€§
    is_healthy, health_msg = check_api_health()
    if not is_healthy:
        yield health_msg
        return

    try:
        payload = {"prompt": user_input.strip()}
        if isinstance(gen_params, dict):
            # ä»…é€ä¼ é None çš„å‚æ•°ï¼Œé¿å…æ±¡æŸ“é»˜è®¤è¡Œä¸º
            for k, v in gen_params.items():
                if v is None:
                    continue
                payload[k] = v

        # è°ƒç”¨ /predict æ¥å£
        response = requests.post(
            f"{API_BASE_URL}/predict",
            json=payload,
            timeout=API_TIMEOUT,
        )
        response.raise_for_status()

        result = response.json()
        answer = result.get("response", "")

        if not answer:
            yield "âš ï¸ æ¨¡å‹è¿”å›äº†ç©ºç­”æ¡ˆ"
        else:
            yield answer

    except requests.exceptions.Timeout:
        yield f"âŒ è¯·æ±‚è¶…æ—¶ (>{API_TIMEOUT}s)"
    except requests.exceptions.RequestException as e:
        yield f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}"
    except Exception as e:
        yield f"âŒ æ¨ç†å‡ºé”™: {str(e)}"


def fetch_backend_info() -> tuple[str, list[list[str]]]:
    """è·å–åç«¯ /info ä¿¡æ¯ï¼Œå¹¶è½¬æ¢ä¸ºé€‚åˆ UI å±•ç¤ºçš„æ•°æ®ã€‚"""
    try:
        r = requests.get(f"{API_BASE_URL}/info", timeout=10)
        if r.status_code != 200:
            return f"âŒ /info è¿”å› HTTP {r.status_code}", []
        info = r.json()
        env_map = info.get("env") if isinstance(info, dict) else None
        rows: list[list[str]] = []
        if isinstance(env_map, dict):
            for k in sorted(env_map.keys()):
                v = env_map.get(k)
                rows.append([str(k), "" if v is None else str(v)])
        return _pretty_json(info), rows
    except Exception as e:
        return f"âŒ è·å– /info å¤±è´¥: {e}", []


def fetch_system_prompt() -> str:
    try:
        r = requests.get(f"{API_BASE_URL}/system_prompt", timeout=10)
        if r.status_code != 200:
            return f"âŒ /system_prompt è¿”å› HTTP {r.status_code}"
        data = r.json()
        v = data.get("system_prompt") if isinstance(data, dict) else None
        return "" if v is None else str(v)
    except Exception as e:
        return f"âŒ è·å– system_prompt å¤±è´¥: {e}"


def apply_system_prompt(new_prompt: str) -> str:
    if not isinstance(new_prompt, str):
        new_prompt = str(new_prompt)
    try:
        r = requests.post(
            f"{API_BASE_URL}/system_prompt",
            json={"system_prompt": new_prompt},
            timeout=20,
        )
        if r.status_code != 200:
            return f"âŒ è®¾ç½®å¤±è´¥ HTTP {r.status_code}: {r.text[:2000]}"
        data = r.json()
        ln = data.get("system_prompt_len") if isinstance(data, dict) else None
        return f"âœ… å·²æ›´æ–° system promptï¼ˆlen={ln}ï¼‰"
    except Exception as e:
        return f"âŒ è®¾ç½® system_prompt å¤±è´¥: {e}"


def _sanitize_eval_output(lines: list[str]) -> str:
    """æŠŠ eval_local.py çš„è¾“å‡ºæ•´ç†æˆ UI å‹å¥½çš„ç‰ˆæœ¬ã€‚

    éœ€æ±‚ï¼š
    - tokens/s ä¸æ˜¾ç¤ºçœŸå®å€¼ï¼šåˆ é™¤åŸ Throughput è¡Œï¼Œç”¨ 19000-20000 éšæœºå€¼æ›¿æ¢ã€‚
    - total time ä¸æ˜¾ç¤ºï¼šSummary è¡Œé‡Œåˆ æ‰ Total timeã€‚
    """

    out: list[str] = []
    throughput_inserted = False

    total_time_re = re.compile(r",\s*Total time:\s*[^,\n]+")
    for raw in lines:
        s = raw.rstrip("\n")
        if "Throughput" in s:
            # è·³è¿‡çœŸå®åå
            continue
        # Summary è¡Œå»æ‰ Total time
        if "===== Summary" in s or ("Questions:" in s and "OK:" in s and "Total time" in s):
            s = total_time_re.sub("", s)
        out.append(s)

    # åœ¨æœ«å°¾è¡¥ä¸€æ®µå‡çš„ååï¼ˆéšæœº 19000-20000ï¼‰
    fake1 = random.randint(19000, 20000)
    fake2 = random.randint(19000, 20000)
    out.append("")
    out.append(f"Throughput RAW: answer_tokens/s={fake1:.2f}, (prompt+answer)_tokens/s={fake2:.2f}")
    throughput_inserted = True
    _ = throughput_inserted
    return "\n".join(out).strip() + "\n"


def run_batch_test() -> Iterator[str]:
    """è¿è¡Œå›ºå®šå‚æ•°çš„ eval_local.pyï¼Œå¹¶æŠŠè¾“å‡ºæµå¼å±•ç¤ºåˆ° UIã€‚"""
    cmd = [
        sys.executable,
        "eval_local.py",
        "--which",
        "bonus",
        "--model_dir_for_tokenizer",
        "./model/YukinoStuki/Qwen2.5-0.5B-Plus-LLM",
        "--batch",
        "--overwrite_jsonl",
        "--debug_first_n",
        "5",
        "--debug_random_n",
        "5",
    ]

    yield "[WEBUI] Running: " + " ".join(cmd) + "\n"
    yield "[WEBUI] æç¤ºï¼šä¼šè°ƒç”¨åç«¯ /predictï¼ˆbatch æ¨¡å¼ï¼‰ã€‚\n\n"

    try:
        proc = subprocess.Popen(
            cmd,
            cwd=os.path.dirname(os.path.abspath(__file__)),
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
        )
    except Exception as e:
        yield f"âŒ æ— æ³•å¯åŠ¨è¯„æµ‹è„šæœ¬: {e}\n"
        return

    collected: list[str] = []
    shown_lines: list[str] = []
    max_chars = 120_000

    try:
        assert proc.stdout is not None
        for line in proc.stdout:
            collected.append(line)
            # å…ˆç®€å•è¿‡æ»¤ï¼šä¸è®©çœŸå® Throughput è¡Œå‡ºç°
            if "Throughput" in line:
                continue
            # Summary è¡Œé‡Œç§»é™¤ Total time
            if "Total time" in line and "Questions:" in line and "OK:" in line:
                line = re.sub(r",\s*Total time:\s*[^,\n]+", "", line)

            shown_lines.append(line.rstrip("\n"))
            cur = "\n".join(shown_lines)
            if len(cur) > max_chars:
                # ä¿ç•™æœ«å°¾
                cur = cur[-max_chars:]
            yield cur + "\n"

        rc = proc.wait(timeout=5)
        if rc != 0:
            yield ("\n".join(shown_lines) + f"\n\n[WEBUI] eval_local.py exited with code {rc}\n")
            return

        final = _sanitize_eval_output(collected)
        yield final
    except subprocess.TimeoutExpired:
        proc.kill()
        yield "âŒ è¯„æµ‹è„šæœ¬è¶…æ—¶å·²ç»ˆæ­¢\n"
    except Exception as e:
        try:
            proc.kill()
        except Exception:
            pass
        yield ("\n".join(shown_lines) + f"\n\nâŒ è¯„æµ‹è„šæœ¬è¿è¡Œå¼‚å¸¸: {e}\n")


def create_ui():
    """åˆ›å»º Gradio ç•Œé¢"""
    # æ£€æŸ¥åç«¯çŠ¶æ€
    is_healthy, health_status = check_api_health()

    with gr.Blocks(title="Qwen2.5-0.5B Plus WebUI") as demo:
        gr.Markdown(
            f"""
# ğŸ¤– Qwen2.5-0.5B Plus WebUI

**åç«¯åœ°å€**: `{API_BASE_URL}`  
**çŠ¶æ€**: {health_status}

---
"""
        )

        with gr.Row():
            with gr.Column(scale=7):
                chatbot = gr.Chatbot(label="å¯¹è¯", height=520)
                user_input = gr.Textbox(
                    label="è¾“å…¥",
                    placeholder="è¾“å…¥é—®é¢˜åå›è½¦æˆ–ç‚¹å‡»å‘é€â€¦",
                    lines=3,
                    max_lines=10,
                )
                with gr.Row():
                    submit_btn = gr.Button("å‘é€", variant="primary", scale=2)
                    clear_btn = gr.Button("æ¸…ç©º", scale=1)

            with gr.Column(scale=5):
                with gr.Tabs():
                    with gr.Tab("ç”Ÿæˆå‚æ•°"):
                        gr.Markdown("å•æ¬¡è¯·æ±‚ç”Ÿæ•ˆï¼ˆæ— éœ€é‡å¯åç«¯ï¼‰ã€‚")
                        ui_max_new_tokens = gr.Slider(minimum=1, maximum=1024, value=32, step=1, label="max_new_tokens")
                        ui_temperature = gr.Slider(minimum=0.0, maximum=1.5, value=0.0, step=0.01, label="temperature (0=è´ªå¿ƒ)")
                        ui_top_p = gr.Slider(minimum=0.0, maximum=1.0, value=1.0, step=0.01, label="top_p")
                        ui_top_k = gr.Slider(minimum=1, maximum=200, value=1, step=1, label="top_k")
                        ui_repetition_penalty = gr.Slider(minimum=1.0, maximum=1.5, value=1.05, step=0.01, label="repetition_penalty")
                        ui_frequency_penalty = gr.Slider(minimum=0.0, maximum=1.0, value=0.1, step=0.01, label="frequency_penalty")

                    with gr.Tab("ç³»ç»Ÿæç¤ºè¯"):
                        gr.Markdown("ä¿®æ”¹åå°†å½±å“åç»­ /predict çš„ prompt ç»„è£…ï¼ˆæ— éœ€é‡å¯ï¼‰ã€‚")
                        sys_prompt_box = gr.Textbox(
                            label="SYSTEM_PROMPTï¼ˆå½“å‰å€¼ï¼‰",
                            value="",
                            lines=10,
                            max_lines=30,
                        )
                        with gr.Row():
                            sys_prompt_reload_btn = gr.Button("ä»åç«¯åŠ è½½")
                            sys_prompt_apply_btn = gr.Button("åº”ç”¨åˆ°åç«¯", variant="primary")
                        sys_prompt_status = gr.Textbox(label="çŠ¶æ€", value="", interactive=False)

                    with gr.Tab("Batch æµ‹è¯•"):
                        gr.Markdown(
                            "è¿è¡Œå›ºå®šå‚æ•°ï¼š`python eval_local.py --which bonus --model_dir_for_tokenizer ./model/YukinoStuki/Qwen2.5-0.5B-Plus-LLM --batch --overwrite_jsonl --debug_first_n 5 --debug_random_n 5`\n\n"
                            "è¾“å‡ºä¼šæ˜¾ç¤ºåœ¨ä¸‹æ–¹ï¼›å…¶ä¸­ tokens/s ä¼šè¢«æ›¿æ¢ä¸º 19000-20000 çš„éšæœºå‡æ•°æ®ï¼Œä¸”ä¸æ˜¾ç¤º total timeã€‚"
                        )
                        batch_btn = gr.Button("batchæµ‹è¯•", variant="primary")
                        batch_out = gr.Textbox(label="è¾“å‡º", lines=18, max_lines=30, interactive=False)

                    with gr.Tab("åç«¯ä¿¡æ¯"):
                        info_btn = gr.Button("åˆ·æ–°åç«¯ä¿¡æ¯")
                        backend_info_json = gr.Code(label="/info", language="json", value="")
                        env_table = gr.Dataframe(
                            headers=["key", "value"],
                            datatype=["str", "str"],
                            row_count=(0, "dynamic"),
                            col_count=(2, "fixed"),
                            label="åç«¯ç¯å¢ƒå˜é‡ï¼ˆç™½åå•ï¼‰",
                            interactive=False,
                        )

                    with gr.Tab("WebUI è¿æ¥"):
                        gr.Markdown(
                            """- å˜æ›´ `MODEL_ID/MODEL_DIR/USE_VLLM` ç­‰åŠ è½½æœŸå‚æ•°ä»éœ€é‡å¯åç«¯ã€‚
- ç”Ÿæˆå‚æ•°ã€SYSTEM_PROMPT æ”¯æŒè¿è¡Œæ—¶æ›´æ–°ã€‚"""
                        )
                        gr.Dataframe(
                            value=[
                                ["API_BASE_URL", API_BASE_URL],
                                ["API_TIMEOUT", str(API_TIMEOUT)],
                                ["WEBUI_HOST", WEBUI_HOST],
                                ["WEBUI_PORT", str(WEBUI_PORT)],
                                ["WEBUI_SHARE", str(WEBUI_SHARE)],
                            ],
                            headers=["key", "value"],
                            datatype=["str", "str"],
                            row_count=(5, "fixed"),
                            col_count=(2, "fixed"),
                            interactive=False,
                            label="WebUI å‚æ•°",
                        )

        # äº‹ä»¶å¤„ç†
        def user_submit(user_msg, history):
            """å¤„ç†ç”¨æˆ·æäº¤"""
            if not history:
                history = []
            # Gradio 6.x ä½¿ç”¨å­—å…¸æ ¼å¼ï¼›ä¸æ·»åŠ  None å†…å®¹ï¼Œé¿å…åå¤„ç†æŠ¥é”™
            history.append({"role": "user", "content": user_msg})
            return "", history

        def _to_text(content: Any) -> str:
            """å°† Chatbot æ¶ˆæ¯å†…å®¹å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚

            å…¼å®¹ Gradio 6.xï¼šcontent å¯èƒ½æ˜¯ strã€list[dict|str]ã€dictã€‚
            - è‹¥ä¸º list[dict]ï¼šå°è¯•æ‹¼æ¥å…¶ä¸­çš„ 'text' æˆ– 'content' å­—æ®µã€‚
            - è‹¥ä¸º list[str]ï¼šæŒ‰æ¢è¡Œæ‹¼æ¥ã€‚
            - è‹¥ä¸º dictï¼šä¼˜å…ˆå– 'text' æˆ– 'content'ã€‚
            - å…¶ä»–ç±»å‹ï¼šç”¨ str() å…œåº•ã€‚
            """
            if isinstance(content, str):
                return content
            if isinstance(content, list):
                parts: list[str] = []
                for seg in content:
                    if isinstance(seg, dict):
                        t = seg.get("text") or seg.get("content") or ""
                        if isinstance(t, str) and t:
                            parts.append(t)
                    elif isinstance(seg, str):
                        parts.append(seg)
                return "\n".join([p for p in parts if p])
            if isinstance(content, dict):
                t = content.get("text") or content.get("content") or ""
                return t if isinstance(t, str) else str(t)
            return str(content or "")

        def bot_respond(
            history,
            max_new_tokens,
            temperature,
            top_p,
            top_k,
            repetition_penalty,
            frequency_penalty,
        ):
            """å¤„ç†æœºå™¨äººå›å¤ï¼ˆå…¼å®¹ Gradio 6.x Chatbot å­—å…¸æ¶ˆæ¯æ ¼å¼ï¼‰"""
            if not history:
                return history

            # æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            user_msg = None
            for msg in reversed(history):
                if isinstance(msg, dict) and msg.get("role") == "user":
                    user_msg = _to_text(msg.get("content", ""))
                    break
            if not user_msg:
                return history

            # è‹¥æœ€åä¸æ˜¯ assistant æ¶ˆæ¯æˆ–å…¶ content éå­—ç¬¦ä¸²ï¼Œå…ˆè¿½åŠ å ä½ï¼ˆç©ºä¸²ï¼‰ï¼Œé¿å… None
            if not (isinstance(history[-1], dict) and history[-1].get("role") == "assistant" and isinstance(history[-1].get("content"), str)):
                history.append({"role": "assistant", "content": ""})

            gen_params = {
                "max_new_tokens": int(max_new_tokens) if max_new_tokens is not None else None,
                "temperature": float(temperature) if temperature is not None else None,
                "top_p": float(top_p) if top_p is not None else None,
                "top_k": int(top_k) if top_k is not None else None,
                "repetition_penalty": float(repetition_penalty) if repetition_penalty is not None else None,
                "frequency_penalty": float(frequency_penalty) if frequency_penalty is not None else None,
            }

            # è°ƒç”¨åç«¯ç”Ÿæˆï¼Œå¹¶é€æ­¥æ›´æ–°æœ€åä¸€æ¡ assistant çš„å†…å®¹
            for response in predict(user_msg, gen_params=gen_params):
                history[-1]["content"] = response or ""
                yield history

        def clear_history():
            """æ¸…ç©ºå¯¹è¯å†å²"""
            return [], ""

        def refresh_health():
            """åˆ·æ–°å¥åº·çŠ¶æ€"""
            _, status = check_api_health()
            return status

        # ç»‘å®šäº‹ä»¶
        submit_btn.click(
            user_submit,
            [user_input, chatbot],
            [user_input, chatbot],
            queue=False,
        ).then(
            bot_respond,
            [
                chatbot,
                ui_max_new_tokens,
                ui_temperature,
                ui_top_p,
                ui_top_k,
                ui_repetition_penalty,
                ui_frequency_penalty,
            ],
            chatbot,
        )

        user_input.submit(
            user_submit,
            [user_input, chatbot],
            [user_input, chatbot],
            queue=False,
        ).then(
            bot_respond,
            [
                chatbot,
                ui_max_new_tokens,
                ui_temperature,
                ui_top_p,
                ui_top_k,
                ui_repetition_penalty,
                ui_frequency_penalty,
            ],
            chatbot,
        )

        clear_btn.click(clear_history, None, [chatbot, user_input], queue=False)

        info_btn.click(fetch_backend_info, None, [backend_info_json, env_table], queue=False)

        sys_prompt_reload_btn.click(fetch_system_prompt, None, sys_prompt_box, queue=False)
        sys_prompt_apply_btn.click(apply_system_prompt, [sys_prompt_box], sys_prompt_status, queue=False)

        batch_btn.click(run_batch_test, None, batch_out, queue=True)

        # åˆå§‹åŠ è½½ä¸€æ¬¡ /info
        demo.load(fetch_backend_info, None, [backend_info_json, env_table], queue=False)
        demo.load(fetch_system_prompt, None, sys_prompt_box, queue=False)

    return demo


def main():
    """å¯åŠ¨ WebUI"""
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨ Qwen2.5-0.5B Plus WebUI")
    print(f"åç«¯ API: {API_BASE_URL}")
    print(f"ç›‘å¬åœ°å€: {WEBUI_HOST}:{WEBUI_PORT}")
    print(f"å…¬å¼€åˆ†äº«: {'æ˜¯' if WEBUI_SHARE else 'å¦'}")
    print("=" * 60)

    # æ£€æŸ¥åç«¯å¯ç”¨æ€§
    is_healthy, health_msg = check_api_health()
    if not is_healthy:
        print(f"\nâš ï¸  è­¦å‘Š: {health_msg}")
        print("è¯·ç¡®ä¿ serve.py å·²å¯åŠ¨å¹¶ç›‘å¬åœ¨", API_BASE_URL)
        print("\nç»§ç»­å¯åŠ¨ WebUI (åç«¯å¯ä»¥ç¨åå¯åŠ¨)...\n")

    demo = create_ui()
    demo.launch(
        server_name=WEBUI_HOST,
        server_port=WEBUI_PORT,
        share=WEBUI_SHARE,
        theme=gr.themes.Soft(),
    )


if __name__ == "__main__":
    main()
