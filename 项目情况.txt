这是一个微调大模型来进行测评的项目，需要选用开源大模型进行微调，把一本书作成问答对作为数据集，然后在测评机上进行测评准确率和token速度。以下是测评要求：
”
2025.12.24 23:54前的部署公钥已添加。着急提交的同学可以直接参考jihulab的添加方式，这样当下就能提交。如果超过一天没有处理，请push一下助教。教育版邀请链接已更新。

12月25日更新的通知：后台检测到一些提交没有邮件反馈和结果记录，这些提交次数将在12月29日统一返还，如果提交后超过1天没有收到邮件，可以尝试重新提交。这种情况很少，一共只有几个。

12月24日更新的通知：在Gitee教育版新fork的仓库默认权限为公开，这个完全没问题，每天晚上助教统一处理帮大家改成私有。评测系统统计速度的方式为：用所有返回的答案的len(str)的总和，除以所有post请求predict接口的总时间和，所以交上去之后tokens/s虚高。

12月23日更新的通知：大致的给分规则如下，如果同时做了加分题和基础题，那就两者取max，具体分数差异还要看速度，速度可观的话得分能超过上一档速度落后的（考虑的所有completed的提交，不只是考虑榜单上的）：加分题准确率大于0.35（满分，可能优秀成果）＞加分题准确率0.28-0.35（部分满分，可能优秀成果，需要评估代码质量）≈基础题大于0.35（部分满分）＞基础题准确率0.23-0.35＞使用了自己微调的模型并尝试了加分题（也会参考准确率和代码质量）＞使用了自己微调的模型并做了基础题（也会参考准确率和代码质量）＞直接交的模板或者没有微调模型

12月23日更新的通知：并行科技的vllm容器使用时可以参考唐湘铸同学提供的教程：https://paste.ubuntu.com/p/dSVjkB2JrK/，若在并行科技的RTX5090机器上面进行调试，建议使用--host网络，否则可能严重timeout。应上级要求，排行榜的赛道信息需要隐去，给同学们带来不便敬请谅解。

12月22日更新的通知： https://docs.qq.com/doc/DT2pNZm13VktMbG9U 微调可以参考。沐曦普通算力券（每人上限2）：http://124.16.71.53:50000/，沐曦vllm算力券（每人上限1）：http://124.16.71.53:50003/，需要校园网访问。同学们微调的时候，用三家任意一家提供的算力都可以，用自己的电脑或者组里的服务器也可以。摩尔券助教这里没有多余的了，可以看看同学们手里有没有多余的，大会领了一些。交的时候也是，自选一个赛道提交即可，不用非得交奖品的那个赛道。

12月22日更新的通知：在最后给成绩时会综合考虑基础题和加分题，只做其中一个或者两个都做都可以。加分题既加分又有机会获得奖品，基础题没有加分没有奖品但是排名靠前也能满分；加分题排名不靠前也能满分，取决于具体加多少分。

12月22日更新的通知： 关于完成大作业二的流程，将自己微调之后的模型上传到modelscope或者hf-mirror等平台，修改大作业二的模板代码接入自己微调之后的模型，进行推理速度优化后，在评测系统提交git地址即可，上榜视为作业完成（大作业一也是），评分会综合考虑代码质量，准确率和速度，准确率小于0.35的都是有效提交。沐曦的模板已经接入了modelscope和vllm，并行科技的vllm可参考教程进行，摩尔线程的推理框架可自由源码编译或选择。请同学们在提交前先在各家的平台做充分的debug（借助curl命令给HTTP服务器发请求）和测试（如果是batch推理，注意传参格式，请参考batchtest模板的commit记录），摩尔线程为autodl的S4000的基础镜像（和微调的不是一个，不要选错），并行科技的为RTX5090服务器（如果没货了可以选其他的，需要注意驱动和GPU硬件以及docker镜像之间的兼容性，网络使用host模式），沐曦为魔力方舟和openhydra平台。

12月19日更新的通知：部署公钥添加好后，会在仓库→管理→部署公钥管理→公钥管理→已启用公钥里面出现，不会出现在个人账号的SSH公钥中。

12月19日更新的通知：有同学反馈准确率评价指标可能会把完全相同的句子判分为0的情况，因为我们课程的主题是优化速度，所以准确率大于0.35的设定只是为了大致约束模型的规范性(经统计测试，这个数值无论是基础题还是加分题都是可以通过正常手段满足的)。并且只有过拟合或者直接map才会出现这样的情况，正常来说是不会输出和ground truth完全一样的结果的。

12月19日更新的通知：再次提醒同学们，这个赛道更换系统（校园网访问http://124.16.71.53:50005/），只是统计同学的奖品偏好，提交的时候交哪个都可以。举个例子，可以只交沐曦的但是拿摩尔线程的奖。最后的排名仅取决于综合总排名（准确率大于0.35的参考，准确率未到的将综合考虑历史提交，不以这个排名为准）。综合总排名的计算方式见之前的内容。

12月18日更新的通知：同学们所有completed的提交记录都将纳入评分范围，如果次数已经用完，榜单上成绩不理想，但是历史提交的completed记录更优的，将也会纳入考虑范围。

12月17日更新的通知：请大家在上一笔提交返回前不要再次提交，同学们如果多次提交的是同一份仓库，那么拉取的是同一份代码，只会在排队轮到的时候拉取代码，拉取时间不定，提交结果没有任何差异，如果两次都是 completed，白浪费一次次数。

12月17日更新的通知：12月29日22点55分前，结果为failed的提交以及准确率小于0.001的提交的次数都将每天晚上全部退还，请大家积极踊跃提交。

12月15日更新的通知：使用batch推理的同学,请确保返回的答案数量和问题的数量一致。

12月15日更新的通知：沐曦的batch提交模式已经上线,三家的batch提交都可以了,具体的接口调用格式可以参考这仨模板的serve.py程序，可以看commit记录查看有哪些改动(请不要直接把这些模板交上去,没有接入模型,会把自己挂在榜上,交模板请交下面的那些):https://gitee.com/gpuap/metax-demo-batchtest, https://edu.gitee.com/gpuap/repos/gpuap/paratera-demo-batchtest, https://edu.gitee.com/gpuap/repos/gpuap/mthreads-demo-batchtest

12月15日更新的通知：当提交的准确率超过0.35（暂定这个数值）之后，原有的成绩将被这个更高准确率的成绩取代，且排名会在所有红色准确率的前面。关于摩尔线程的低版本驱动上vllm框架的源码编译，感兴趣的同学可以自行尝试，工程师提示要使用mp_22编译。

12月13日更新的通知：关于提交前的本地调试：serve.py执行后会启动一个HTTP服务器，它会在指定的端口侦听，可以使用curl命令给这个服务器的'/'端点发送GET请求，给/predict'端点发送POST请求，然后测试是否能够正常工作。可以使用netstat -tunlpe来查询当前侦听的地址和端口。autodl容器实例和并行科技的云服务器上都可以用这种方式调试，沐曦也可参考相关手册进行调试，这样能和评测系统的环境严格保持一致。若有条件，本地进行并行科技容器的构建和运行也可以和评测系统严格保持一致（需要是较新型号的显卡，并且本地的GPU驱动大于等于容器镜像本身要求的）。摩尔线程评测系统的base环境和autodl新建的容器实例的base环境完全一致，可以参考。

12月13日更新的通知：关于batch推理的补充说明：在健康检查的‘/’端点return {"status": "batch"}时开启batch模式，开启后，将会把所有评测数据一次性全部推送到‘predict’端点，第一次就全部发过来（不再需要先传密钥），全程只会调用一次，自己可以基于这个逻辑在predict内部做batch或者做多线程，粒度自定，然后需要在超时之前将全部的结果一并返回。

12月13日更新的重要通知：由于我们的最后一节课提前到了2026年1月8日，为了给排名靠前的同学留出准备展示的时间，因此大作业一和二的截止时间需要做出调整，调整如下：2026年1月5日23点55分将会把所有人的大作业二的可提交次数清零，2026年1月7日16点00分将会把大作业一和大作业二的所有排队作业清空，同时大作业一的可提交次数清零。Gitee教育版的邀请链接刷新了，miss的同学可以继续加入。12日同学们反馈的Bug均已修复完毕，failed的提交次数也已经全部重置。沐曦赛道的Batch提交功能也将于下周一上线。并行科技的vllm latest镜像是可用的，已经有同学基于这个镜像成功构建提交；为了满足同学们对固定版本vllm的需求，也新追加了一个镜像，同时并行科技的上线等待时间调整为了420秒。对于Batch版本的提交格式，可以参考群里发的示例程序。

12月12日更新的通知：之前failed的提交次数已经退还，考虑到大作业二调试难度大，failed不占提交次数的活动将延期到12月29日，中间会多次进行次数退还。提醒同学们至少需要把推理速度提升到300tokens/s以上才有机会做完所有的题目，建议使用batch推理或者并发推理或者参数量化等技术来提速。

12月11日更新的通知：在并行科技的服务器上拉取modelscope的速度要比拉取huggingface国内源的速度快很多，同学们可以优先使用modelscope。另外由于提交次数是基础题和加分题共享的，想评奖的同学可以直接冲加分题，不用先交基础题。

12月11日更新的通知：为了更好地帮助同学们排查问题，如果对run阶段的错误无法解决需要提供日志详情的，请给助教发email:gonghao20@mails.ucas.ac.cn，注明提交id，将在每天晚上统一回复。如果有漏的，麻烦push一下助教。

12月10日更新的通知：为了满足同学们对batch推理的需求，我们决定再给评测系统加一个小patch。对于第一个predict的调用请求，如果直接返回一个固定的密钥，那么第二次调用predict的时候，将会把全部的问题一次性推送，需要在这第二次predict调用一次性全部处理完毕，并在规定时间内返回。如果超时将导致直接失败，不会再有第三次predict调用了。以前的串行predict逻辑依然适用。这个patch将在未来的一两天内开发完毕，到时候推送给同学们具体细节，在这个期间，现有系统的任何提交都不受影响。对于不考虑batch调用的同学，可以完全忽略这条消息。

12月10日更新的通知：模板代码中健康检查端点请保留，评测系统会先去访问这个接口，只有这个接口在指定时间上线之后才会开始访问predict接口。健康检查的时间可以做准备工作，包括模型预热等。

12月9日更新的通知：考虑到很多同学因提交次数限制不敢尝试提交，且担心交上去中间环节报错，经慎重考虑，12月15日23点前所有评测结果为failed的提交次数全部归还，在12月15日当天统一处理。同学们在这个时间之前敞开了交就行了，留给大家熟悉系统环境用。

12月9日更新的通知：为了避免提交失败，同学们需要同时考虑网络带宽，硬盘余量，时间限制，这三个因素在三家的README.md文件中有详细说明；还需要考虑题量和准确率的计算方式。在提交前建议先在各家的平台上充分测试软件环境构建方案可行性（比如并行科技如果直接在requirements.txt里面pip install vllm会出问题），摩尔线程的评测机在AutoDL的摩尔线程专区的基础镜像容器实例上，并行科技的评测机在并行科技的云服务器上，沐曦的评测机在沐曦的裸金属服务器上。前两者的软件环境调试可以直接租用对应平台的算力，沐曦的软件环境调试可以参考发给同学们的手册和文档。

本评测系统由中国科学院大学与摩尔线程、沐曦和并行科技三家公司共建（排名不分先后）。本评测系统继承了大作业一的邮箱、验证码和昵称。若还没有完成大作业一的验证码获取，请先去大作业一获取验证码后再使用本系统。本系统的提交次数设定为每人20次。2026年1月5日23点55分将会把所有人的可提交次数清零，2026年1月7日16点00分将会把所有排队作业清空。只有当评测排队到同学们的提交的时候才会实时拉取代码仓库，因此请避免提交后改仓库，或者可以使用多份代码仓库（记得加部署公钥）。

最终的评奖排名将有1/5取决于大作业一的加分题，4/5取决于大作业二的加分题。

使用本系统时，若遇到评测系统本身的BUG，请联系龚昊助教；三家都提供了模板仓库，模板仓库可以直接提交并上榜，若因修改了模板仓库而导致的任何报错，请先自查，若仍不能解决，再根据报错的具体原因在群里提问、联系助教或在三家的答疑群提问。build阶段的日志将会完整呈现，run阶段的stdout和stderr都将隐藏。

以下是三家的模板仓库，可以fork或者新建仓库复用，这些仓库保证能直接提交就能上榜：

摩尔线程模板仓库 git@gitee.com:gpuap/mthreads-demo.git
沐曦模板仓库 git@gitee.com:gpuap/metax-demo.git
并行科技模板仓库 git@gitee.com:gpuap/paratera-demo.git
所提交的Git仓库地址必须是SSH地址，且已经将下述公钥设为部署公钥：(这个自己添加不了，会撞，使用Gitee教育版的同学将由助教每天晚上睡觉前统一给大家添加，当天新建的仓库请在第二天提交)。对于未使用Gitee教育版的同学，由于很多平台不支持不同的账号添加相同的公钥，因此可以通过将助教邀请到代码仓库中来解决这个问题。对于Gitee平台：授予管理员权限（开发者权限不够，必须管理员），在Gitee平台中助教的邮箱为aininot260@126.com(添加路径为：代码仓库→管理→仓库成员管理→管理员→邀请用户→直接添加)，添加完了之后等第二天应该就有部署公钥了。对于极狐Gitlab（gitlab.cn，非Gitlab国际版）：助教的id为aininot260(添加路径为：管理→成员→邀请成员)，设置最大角色为所有者，这个应该可以直接提交了，不用等第二天。Gitlab应该隐私性更强一些，连仓库地址也不会公开。

ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILRZTwyAASc4IRGSVH/bCTYJ2pdG0fHoFntQ8I7gPvFT Gitee SSH Key

部署公钥的教程参见：

https://gitee.com/help/articles/4181#article-header0
如果不设置，会导致评测系统拉取失败，浪费提交次数。请同学们将自己的代码仓库的权限设置为Private。

只要有上榜的提交，就可以获得大作业二的课程分数，但是具体得多少分，仅取决于综合总排名。大作业二进行过程中，及格准确率（准确率未标红的）会根据同学们的模型质量进行动态调整(目前为0.35)，但不及格的准确率（准确率标红的）依然能够获得课程的分数（具体分数差异会根据最后同学们提交的质量做动态调整），如果准确率低于0.001，将无法上榜。

build阶段，可以访问Internet；run阶段，不允许访问Internet。

由于代码仓库的容量有限，仓库不一定能够容纳完整的模型权重，因此可以在容器的build阶段将自己的权重准备好，请参考三家的代码模板完成这一步。

由于Gitee教育版的容量和总人数都有限制，因此完全可以使用个人的Gitee的私有仓库完成作业，使用极狐GitLab等其它平台也可以，但谨慎使用Github平台，因为网络问题可能拉取失败，浪费提交次数。不保证评测系统的网络能访问Github。我们为同学们提供了Gitee教育版，其总人数上限为200，仓库总容量为20GB，附件总容量为10GB，LFS总容量为1GB（是200人的总容量）： 点击加入课程专属Gitee教育版班级

每位同学，无论选择的是什么赛道，都可以向所有赛道提交自己的程序，同学们选择的赛道仅会决定最终获得的奖励是哪家公司提供的（即提交到哪个赛道和获得哪个赛道的奖品是两码事，比如可以在沐曦成绩最好但是拿摩尔的奖品），但是请注意不要出现类似把仅兼容沐曦的代码仓库提交到摩尔线程这种情况，一定会报错。最后同学们的成绩取决于综合总排名。综合总排名的计算方式为：取每位同学在所有赛道的成绩中，本赛道排名/本赛道的提交总人数，3个比值中比值最小的那个，并按从小到大的顺序排序。本赛道的提交总人数里面，即使是准确率不及格的也会作为分母，只要在榜上的都算分母。并且每位同学仅能获得在课程开始时所选的赛道的奖品。举个例子，比如同学A选的沐曦赛道，但是拿了第一名，那他最终也只能获得沐曦赛道的奖品；同学B选的摩尔线程赛道，但是排最后一名，那么他无法获得任何奖品。另外，只有加分题赛道有成绩的同学，才有机会获得奖品。（沐曦赛道除外，沐曦赛道只要选了就有奖品，无论是否做加分题）加分题指的是给大作业二的课程成绩加分。我们开发了一个赛道更换系统（校园网访问http://124.16.71.53:50005/），方便同学们随时更换赛道,若无法访问校园网可私聊助教发送验证码和要更入的赛道，每位同学的赛道信息将被隐藏，同学们互相不知道其他人换到了什么赛道里面。沐曦赛道初步设定最多100个人（后期可以协商增加），将在赛道更换系统进行实时限制；另外两个赛道的同学，看加分题的名次决定是否能获得奖品。

由于涉及到环境构建和推理运行前的准备阶段，需要严格控制每次提交在各个阶段的时间，这些时间将根据情况进行动态调整，具体请见各家的代码模板的README.md。任何一个阶段超时评测都将终止。因此，请精细地分配所使用的模型的大小，避免因模型过大导致下载超时。三家的评测平台均不会进行任何的缓存

非batch推理得推理阶段超时不会导致整个提交失败，但是会把后面所有没有完成的题目的答案标记为""。batch推理超时将直接失败。基础题有292道问答对，加分题有358道问答对，因此，优先提高速度，才能完成尽可能多的题目，才有机会拿到更高的准确率。评测准确率所使用的函数如下： 点击查看准确率评测函数
“
评测规则如下：
“
评测系统调用接口（API 契约）

容器启动后必须提供：

GET /：健康检查。评测系统会先访问该接口，只有健康检查通过才会调用 predict。

POST /predict：推理接口，请求 JSON：

{"prompt":"..."}


返回 JSON：

{"response":"..."}


端口：Dockerfile 的 EXPOSE 端口不要随意改（模板默认 8000）。

2) 运行环境/限制

build 阶段允许联网（可下载依赖/权重）。

run 阶段不允许访问 Internet。

judge 配置（沐曦）：Ubuntu 24.04，24 CPU / 200GB RAM / 1TB disk / GPU MXC500 64GB，网络 100Mbps。

时间限制（模板 README）：build 900s，health 180s，predict 360s。

3) 题量与得分逻辑

基础题 292， 加分题 358。

评分综合考虑：准确率 + 速度 + 代码质量（最终综合总排名）。

准确率阈值动态但当前常提 0.35；小于 0.001 不上榜。准确率<0.35仍算有效提交但分数可能低。

batch 推理可提速，但 batch 超时会直接失败；非 batch 超时会把后续答案置空。
"
Fork 了沐曦模板仓库（metax-demo），并已按要求添加了部署公钥（用于评测系统拉取）。

已微调并得到 LoRA/adapter，之后合并（merge）得到完整权重目录（本地测试跑通）。

已将合并后的模型上传到了 ModelScope（仓库：YukinoStuki/Qwen3-4B-Plus-LLM-AWQ，可见链接在上传日志中显示创建成功）。

评测机已提交过一次：准确率 0.2035、速度 123.43 tokens/s（不达标，需提升准确率≥0.35 以及速度）。

已写本地评测脚本（eval_local.py），可以在本地/云服务器对 serve.py 服务做自动测试并复现评测方式。

测评总计20次机会

模板核心文件

Dockerfile：基于沐曦提供的 vLLM 镜像（FROM 不建议改）。build 阶段下载模型到 /app/model/...

download_model.py：在 build 阶段从 ModelScope 拉取模型权重到本地目录

serve.py：核心推理服务，提供 GET / 和 POST /predict

模型目前选用Qwen3-4B，数据集还可以重新生成，重新微调

ModelScope 仓库

合并后的模型已上传到 ModelScope：YukinoStuki/Qwen3-4B-Plus-LLM-AWQ（master）

download_model.py 在 Docker build 阶段会把这个仓库拉到 ./model/YukinoStuki/Qwen3-4B-Plus-LLM-AWQ 或类似目录。

4. 评测准确率计算函数（必须对齐）

评测机准确率函数（RougeL-F1，jieba 分词）：

对 pred/ref 分别 jieba.lcut 再用 RougeL scorer 计算 fmeasure

pred/ref 空则该条记 0

若 predictions 数量少于 ground_truth，会补空串

平均分为最终 accuracy

我们本地 eval_local.py 复现了这个逻辑，用 docx 测试集读取 QA，逐题调用服务端 /predict，并计算 RougeL 平均分。

目前在本地和云服务器上双方向测试，最终评测机次数有限，本地和云测试通过有较大提升后尝试最终评测机

本地和云均无法运行docker，最终测评机通过docker运行。

本地和云直接运行serve.py或uvicorn serve:app --host 0.0.0.0 --port 8000，通过curl http://127.0.0.1:8000/ 完成健康检测

通过eval_local.py用两个docx题目可以对模型就进行云平台测试，run.sh为测试命令

旧模型性能：评测机：accuracy 0.3651，speed 5124 tokens/s 本地复现（旧代码）：accuracy ~0.86（示例 100题），吞吐约 3965 tokens/s

目前追求完成加分题项目，需要重新微调模型，并上传，测试

目标 准确率达到 ≥0.35 速度尽量 >300 tokens/s（越高越好）

仓库与地址：

Gitee（评测用）：git@gitee.com:yukinostuki/metax-demo.git（master）

GitHub（镜像/Codex 用）：git@github.com:YukinoStuki2/metax-demo-mirror.git（master）

ModelScope 模型：YukinoStuki/Qwen3-4B-Plus-Merged（已上传）

云平台和评测机均为曦云C500 64GB显存，而本地WSL是4090-laptop，写代码请考虑两端兼容，如果实在不行请首要兼容评测机的C500
